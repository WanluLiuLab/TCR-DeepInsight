{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed105fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /rsch/Snowxue/TCR-DeepInsight/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772dba0",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcr_deep_insight.model.modeling_bert._model import (\n",
    "    TRabModelingBertForPseudoSequence,\n",
    "    TCRpMHCPairAttention,\n",
    "    TRabModelingBertForPseudoSequenceWithContactModule\n",
    ")\n",
    "from tcr_deep_insight.model.tokenizers._tokenizer import (\n",
    "    tokenize_tcr_pseudo_sequence_to_fixed_length, \n",
    "    trab_tokenizer_for_pseudosequence,\n",
    "    tokenize_to_fixed_length\n",
    ")\n",
    "from tcr_deep_insight.model.modeling_bert._collator import AminoAcidsCollator\n",
    "from tcr_deep_insight.model.modeling_bert._config import get_human_config\n",
    "\n",
    "\n",
    "from tcr_deep_insight.utils._tcr_definitions import (\n",
    "    _get_hla_pseudo_sequence,\n",
    "    blosum_align,\n",
    "    HLA_I_PSEUDO_INDEX,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pickle\n",
    "import numpy as np \n",
    "from tcr_deep_insight.utils._tcr_definitions import (\n",
    "    _get_hla_pseudo_sequence,\n",
    "    blosum_align,\n",
    "    HLA_I_PSEUDO_INDEX,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "hla_pseudo_sequence = _get_hla_pseudo_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_binding = pd.read_parquet(\"./data/tcrpmhc_pairing.parquet\")\n",
    "\n",
    "df_scrna = pd.read_parquet(\"./data/20240401_huARdb_v2_5.obs.parquet\")\n",
    "\n",
    "import datasets\n",
    "ds_scrna = datasets.load_from_disk(\"./data/datasets/scrna_dataset_TCRab/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrna['bvpseudo'] = list(map(lambda x: ':'.join(x.split(\":\")[:2]), df_scrna['pseudosequence']))\n",
    "df_scrna_bvpseudo_index = {bvpseudo:np.argwhere(np.array(df_scrna['bvpseudo'] == bvpseudo)).flatten() for bvpseudo in np.unique(df_scrna['bvpseudo'])}\n",
    "df_scrna['avpseudo'] = list(map(lambda x: ':'.join(x.split(\":\")[3:5]), df_scrna['pseudosequence']))\n",
    "df_scrna_avpseudo_index = {avpseudo:np.argwhere(np.array(df_scrna['avpseudo'] == avpseudo)).flatten() for avpseudo in np.unique(df_scrna['avpseudo'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60204c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_human_config(bert_type=\"small\", vocab_size=36)\n",
    "model = TRabModelingBertForPseudoSequenceWithContactModule(config).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264fc81",
   "metadata": {},
   "source": [
    "## Generating contact dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11741fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_dataset = []\n",
    "\n",
    "for i in list(filter(lambda x: x.endswith('.pkl'), os.listdir(\"./data/processed_pairing_data/\"))):\n",
    "    with open(\"./data/processed_pairing_data/\" + i, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    if 'tcr_pmhc_pseudo_distogram' not in data:\n",
    "        continue\n",
    "\n",
    "    pmhc_pseudoseq = hla_pseudo_sequence[\"A*02:01\"][\"pseudosequence\"] + ':' + data['peptide']\n",
    "    \n",
    "    tcr_pseudoseq = ':'.join([\n",
    "        data[k] for k in ['cdr1a','cdr2a', 'cdr3a', 'cdr1b','cdr2b', 'cdr3b']\n",
    "    ])\n",
    "\n",
    "    tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "    tcr_input_ids = np.array(tcr_input_ids)\n",
    "    tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "\n",
    "    pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "    pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "    pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "\n",
    "    contact_dataset.append({\n",
    "        \"tcr_sequence\": tcr_sequence,\n",
    "        \"tcr_input_ids\": tcr_input_ids,\n",
    "        \"tcr_attention_mask\": tcr_attention_mask,\n",
    "        \"pmhc_sequence\": pmhc_sequence,\n",
    "        \"pmhc_input_ids\": pmhc_input_ids,\n",
    "        \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "        \"distogram\": data['tcr_pmhc_pseudo_distogram'],\n",
    "        \"pdb_id\": i\n",
    "    })\n",
    "\n",
    "\n",
    "collator = AminoAcidsCollator(\n",
    "    mask_token_id=4,\n",
    "    max_length=100,\n",
    "    mlm_probability=0.15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_contact_data.pkl\",'wb+') as f:\n",
    "    pickle.dump(contact_dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_contact_data.pkl\",'rb') as f:\n",
    "    contact_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273201fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f8498",
   "metadata": {},
   "source": [
    "## Split Training and Testing binding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "agg = df_binding.groupby(\"peptide\").agg({\n",
    "    'tcr_pseudosequence': list,\n",
    "    'pmhc_pseudosequence': list,\n",
    "})\n",
    "train_test_indices = list(map(lambda x: train_test_split(range(len(x)),test_size=0.2),agg['tcr_pseudosequence']))\n",
    "agg_train = pd.DataFrame(\n",
    "    list(\n",
    "        map(\n",
    "            lambda x: (list(np.array(x[1])[x[0][0]]), list(np.\n",
    "array(x[2])[x[0][0]])),\n",
    "            zip(\n",
    "                train_test_indices,\n",
    "                agg[\"tcr_pseudosequence\"],\n",
    "                agg[\"pmhc_pseudosequence\"],\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    index=agg.index,\n",
    "    columns=agg.columns\n",
    ")\n",
    "agg_test = pd.DataFrame(\n",
    "    list(\n",
    "        map(\n",
    "            lambda x: (list(np.array(x[1])[x[0][1]]), list(np.\n",
    "array(x[2])[x[0][1]])),\n",
    "            zip(\n",
    "                train_test_indices,\n",
    "                agg[\"tcr_pseudosequence\"],\n",
    "                agg[\"pmhc_pseudosequence\"],\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    index=agg.index,\n",
    "    columns=agg.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/20240430_agg_train.pkl\",\"wb+\") as f:\n",
    "    pickle.dump(agg_train,f)\n",
    "with open(\"./data/20240430_agg_test.pkl\",\"wb+\") as f:\n",
    "    pickle.dump(agg_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/20240430_agg_train.pkl\",\"rb\") as f:\n",
    "    agg_train = pickle.load(f)\n",
    "with open(\"./data/20240430_agg_test.pkl\",\"rb\") as f:\n",
    "    agg_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec26af",
   "metadata": {},
   "source": [
    "## Training contact task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b54ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for _ in range(50):\n",
    "    epoch_loss = 0\n",
    "    epoch_contact_loss = 0\n",
    "    for i in range(0, len(contact_dataset)):\n",
    "        \n",
    "        tcr_input_ids = torch.from_numpy(contact_dataset[i]['tcr_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.from_numpy(contact_dataset[i]['tcr_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.from_numpy(contact_dataset[0]['pmhc_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.from_numpy(contact_dataset[i]['pmhc_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        distogram = torch.from_numpy(contact_dataset[i]['distogram']).unsqueeze(0).to(\"cuda:0\")\n",
    "\n",
    "        output1 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_distogram=distogram,\n",
    "        )\n",
    "        \n",
    "        loss = output1['contact_loss']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_contact_loss += output1['contact_loss'].item()\n",
    "        \n",
    "    print(epoch_loss, epoch_contact_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16b902",
   "metadata": {},
   "source": [
    "## Training binding task only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "for _ in range(30):\n",
    "    epoch_loss = 0\n",
    "    epoch_binding_loss = 0\n",
    "    epoch_contact_loss = 0\n",
    "    \n",
    "    indices = list(map(lambda x: np.random.choice(range(len(x)), len(x), replace=False), agg_train['tcr_pseudosequence']))\n",
    "    binding_data = []\n",
    "    batch_binding_data = FLATTEN([list(zip(np.array(x)[i], np.array(y)[i])) for x,y,i in zip(\n",
    "        agg_train['tcr_pseudosequence'],\n",
    "        agg_train['pmhc_pseudosequence'], \n",
    "        indices\n",
    "    )])\n",
    "\n",
    "    for tcr_pseudoseq,pmhc_pseudoseq in batch_binding_data:\n",
    "        tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "        pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "        tcr_input_ids = np.array(tcr_input_ids)\n",
    "        tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "        pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "        pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "        binding_data.append({\n",
    "            \"tcr_input_ids\": tcr_input_ids,\n",
    "            \"tcr_attention_mask\": tcr_attention_mask,\n",
    "            \"pmhc_sequence\": pmhc_sequence,\n",
    "            \"pmhc_input_ids\": pmhc_input_ids,\n",
    "            \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "            \"binding\": 1\n",
    "        })\n",
    "\n",
    "        avpseudo = ':'.join(tcr_pseudoseq.split(\":\")[:2])\n",
    "        bvpseudo = ':'.join(tcr_pseudoseq.split(\":\")[3:5])\n",
    "        if avpseudo in df_scrna_avpseudo_index:\n",
    "            indices = df_scrna_avpseudo_index[avpseudo]\n",
    "            if len(indices) > 0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "        if bvpseudo in df_scrna_bvpseudo_index:\n",
    "            indices = df_scrna_bvpseudo_index[bvpseudo]\n",
    "            if len(indices) > 0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "        if avpseudo in df_scrna_avpseudo_index and bvpseudo in df_scrna_bvpseudo_index:\n",
    "            indices = list(set(df_scrna_avpseudo_index[avpseudo]).intersection(set(df_scrna_bvpseudo_index[bvpseudo])))\n",
    "            if len(indices) > 0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "\n",
    "    batch_decoy = ds_scrna[np.random.choice(np.arange(len(ds_scrna)), len(batch_binding_data), replace=False)]\n",
    "\n",
    "    for tcr_input_ids, tcr_attention_mask, pmhc_pseudoseq in zip(batch_decoy['input_ids'],batch_decoy['attention_mask'],list(map(lambda x: x[1], batch_binding_data))):\n",
    "        tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "        binding_data.append({\n",
    "            \"tcr_input_ids\": tcr_input_ids,\n",
    "            \"tcr_attention_mask\": tcr_attention_mask,\n",
    "            \"pmhc_sequence\": pmhc_sequence,\n",
    "            \"pmhc_input_ids\": pmhc_input_ids,\n",
    "            \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "            \"binding\": 0\n",
    "        })\n",
    "    batch_binding_dataset = datasets.Dataset.from_pandas(pd.DataFrame(binding_data))\n",
    "    batch_binding_dataset = batch_binding_dataset.shuffle()\n",
    "            \n",
    "\n",
    "    n_per_batch = 10\n",
    "    for i in tqdm.trange(0, len(batch_binding_dataset) // n_per_batch):\n",
    "        \n",
    "        tcr_input_ids = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['tcr_input_ids']).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['tcr_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['pmhc_input_ids']).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['pmhc_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        binding =  torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['binding']).to(\"cuda:0\").to(torch.float32).unsqueeze(1)\n",
    "        \n",
    "        output2 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_binding=binding    \n",
    "        )\n",
    "\n",
    "        loss = output2['binding_loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_binding_loss += output2['binding_loss'].item()\n",
    "        del output2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss +=  epoch_binding_loss\n",
    "        \n",
    "    print(epoch_loss, epoch_binding_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4492a7f",
   "metadata": {},
   "source": [
    "## Training both contact and binding task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLATTEN(x): return [i for s in x for i in s]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "for _ in range(24):\n",
    "    epoch_loss = 0\n",
    "    epoch_binding_loss = 0\n",
    "    epoch_contact_loss = 0\n",
    "\n",
    "    indices = list(map(lambda x: np.random.choice(range(len(x)), len(x), replace=False), agg_train['tcr_pseudosequence']))\n",
    "\n",
    "    binding_data = []\n",
    "    batch_binding_data =  FLATTEN([list(zip(np.array(x)[i], np.array(y)[i])) for x,y,i in zip(\n",
    "        agg_train['tcr_pseudosequence'],\n",
    "        agg_train['pmhc_pseudosequence'], \n",
    "        indices\n",
    "    )])\n",
    "\n",
    "    for tcr_pseudoseq,pmhc_pseudoseq in batch_binding_data:\n",
    "        tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "        pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "        tcr_input_ids = np.array(tcr_input_ids)\n",
    "        tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "        pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "        pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "        binding_data.append({\n",
    "            \"tcr_input_ids\": tcr_input_ids,\n",
    "            \"tcr_attention_mask\": tcr_attention_mask,\n",
    "            \"pmhc_sequence\": pmhc_sequence,\n",
    "            \"pmhc_input_ids\": pmhc_input_ids,\n",
    "            \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "            \"binding\": 1\n",
    "        })\n",
    "\n",
    "        avpseudo = ':'.join(tcr_pseudoseq.split(\":\")[:2])\n",
    "        bvpseudo = ':'.join(tcr_pseudoseq.split(\":\")[3:5])\n",
    "        if avpseudo in df_scrna_avpseudo_index:\n",
    "            indices = df_scrna_avpseudo_index[avpseudo]\n",
    "            if len(indices)  0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "        if bvpseudo in df_scrna_bvpseudo_index:\n",
    "            indices = df_scrna_bvpseudo_index[bvpseudo]\n",
    "            if len(indices) > 0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "        if avpseudo in df_scrna_avpseudo_index and bvpseudo in df_scrna_bvpseudo_index:\n",
    "            indices = list(set(df_scrna_avpseudo_index[avpseudo]).intersection(set(df_scrna_bvpseudo_index[bvpseudo])))\n",
    "            if len(indices) > 0:\n",
    "                decoy_data = ds_scrna[int(np.random.choice(indices))]\n",
    "                tcr_input_ids = decoy_data['input_ids']\n",
    "                tcr_attention_mask = decoy_data['attention_mask']\n",
    "                binding_data.append({\n",
    "                    \"tcr_input_ids\": tcr_input_ids,\n",
    "                    \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                    \"pmhc_sequence\": pmhc_sequence,\n",
    "                    \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                    \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                    \"binding\": 0\n",
    "                })\n",
    "\n",
    "    batch_decoy = ds_scrna[np.random.choice(np.arange(len(ds_scrna)), len(batch_binding_data), replace=False)]\n",
    "\n",
    "    for tcr_input_ids, tcr_attention_mask, pmhc_pseudoseq in zip(batch_decoy['input_ids'],batch_decoy['attention_mask'],list(map(lambda x: x[1], batch_binding_data))):\n",
    "        tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "        binding_data.append({\n",
    "            \"tcr_input_ids\": tcr_input_ids,\n",
    "            \"tcr_attention_mask\": tcr_attention_mask,\n",
    "            \"pmhc_sequence\": pmhc_sequence,\n",
    "            \"pmhc_input_ids\": pmhc_input_ids,\n",
    "            \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "            \"binding\": 0\n",
    "        })\n",
    "    batch_binding_dataset = datasets.Dataset.from_pandas(pd.DataFrame(binding_data))\n",
    "    batch_binding_dataset = batch_binding_dataset.shuffle()\n",
    "\n",
    "    n_per_batch = min(10, len(batch_binding_dataset) // len(contact_dataset))\n",
    "    batch_contact_dataset = np.array(contact_dataset)[\n",
    "        np.vstack(\n",
    "            [\n",
    "                np.random.choice(range(len(contact_dataset)), 1)\n",
    "                for _ in range(len(batch_binding_dataset) // n_per_batch)\n",
    "            ]\n",
    "        ).flatten()\n",
    "    ]\n",
    "    for i in tqdm.trange(0, len(contact_dataset)):\n",
    "        tcr_input_ids = torch.from_numpy(contact_dataset[i]['tcr_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.from_numpy(contact_dataset[i]['tcr_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.from_numpy(contact_dataset[0]['pmhc_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.from_numpy(contact_dataset[i]['pmhc_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        distogram = torch.from_numpy(contact_dataset[i]['distogram']).unsqueeze(0).to(\"cuda:0\")\n",
    "\n",
    "        output1 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_distogram=distogram,\n",
    "        )\n",
    "\n",
    "        loss = output1['contact_loss'] \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_contact_loss += output1['contact_loss'].item()\n",
    "        del output1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tcr_input_ids = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['tcr_input_ids']).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['tcr_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['pmhc_input_ids']).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['pmhc_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        binding =  torch.tensor(batch_binding_dataset[i*n_per_batch:i*n_per_batch+n_per_batch]['binding']).to(\"cuda:0\").to(torch.float32).unsqueeze(1)\n",
    "\n",
    "        output2 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_binding=binding    \n",
    "        )\n",
    "\n",
    "        loss = output2['binding_loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_binding_loss += output2['binding_loss'].item()\n",
    "        del output2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss += epoch_contact_loss + epoch_binding_loss\n",
    "\n",
    "    print(epoch_loss, epoch_contact_loss, epoch_binding_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47283101",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./20240418_tmp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./20240418_tmp.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([trab_tokenizer_for_pseudosequence.id_to_token(x) for x in batch_binding_dataset[i*n_per_batch+2]['tcr_input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1['contact_output'][1].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=createSubplots(1,2,figsize=(10,5))\n",
    "contact_groud_truth = distogram[0].topk(1)[1].squeeze().detach().cpu().numpy()\n",
    "contact_groud_truth[output1['contact_output'][1].detach().cpu().numpy()[0]==0]=64\n",
    "sns.heatmap(\n",
    "    contact_groud_truth,\n",
    "    ax=axes[0]\n",
    ")\n",
    "contact_prediction = output1['contact_output'][2][0].topk(1)[1].squeeze().detach().cpu().numpy()\n",
    "# contact_prediction[output1['contact_output'][1].detach().cpu().numpy()[0]==0]=64\n",
    "sns.heatmap(\n",
    "    contact_prediction,\n",
    "    ax=axes[1]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fa368",
   "metadata": {},
   "source": [
    "## Train Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binding_predictions = []\n",
    "\n",
    "indices = list(map(lambda x: np.random.choice(range(len(x)), len(x), replace=False), agg_train['tcr_pseudosequence']))\n",
    "binding_data = []\n",
    "batch_binding_data =  FLATTEN([list(zip(np.array(x)[i], np.array(y)[i])) for x,y,i in zip(\n",
    "    agg_train['tcr_pseudosequence'],\n",
    "    agg_train['pmhc_pseudosequence'], \n",
    "    indices\n",
    ")])\n",
    "\n",
    "for tcr_pseudoseq,pmhc_pseudoseq in batch_binding_data:\n",
    "    tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "    pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "    tcr_input_ids = np.array(tcr_input_ids)\n",
    "    tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "    pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "    pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "    binding_data.append({\n",
    "        \"tcr_input_ids\": tcr_input_ids,\n",
    "        \"tcr_attention_mask\": tcr_attention_mask,\n",
    "        \"pmhc_sequence\": pmhc_sequence,\n",
    "        \"pmhc_input_ids\": pmhc_input_ids,\n",
    "        \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "        \"binding\": 1\n",
    "    })\n",
    "\n",
    "    avpseudo = ':'.join(tcr_pseudoseq.split(\":\")[:2])\n",
    "    bvpseudo = ':'.join(tcr_pseudoseq.split(\":\")[3:5])\n",
    "    if avpseudo in df_scrna_avpseudo_index:\n",
    "        indices = df_scrna_avpseudo_index[avpseudo]\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "    if bvpseudo in df_scrna_bvpseudo_index:\n",
    "        indices = df_scrna_bvpseudo_index[bvpseudo]\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "    if avpseudo in df_scrna_avpseudo_index and bvpseudo in df_scrna_bvpseudo_index:\n",
    "        indices = list(set(df_scrna_avpseudo_index[avpseudo]).intersection(set(df_scrna_bvpseudo_index[bvpseudo])))\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "\n",
    "batch_decoy = ds_scrna[np.random.choice(np.arange(len(ds_scrna)), len(batch_binding_data), replace=False)]\n",
    "\n",
    "for tcr_input_ids, tcr_attention_mask, pmhc_pseudoseq in zip(batch_decoy['input_ids'],batch_decoy['attention_mask'],list(map(lambda x: x[1], batch_binding_data))):\n",
    "    tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "    binding_data.append({\n",
    "        \"tcr_input_ids\": tcr_input_ids,\n",
    "        \"tcr_attention_mask\": tcr_attention_mask,\n",
    "        \"pmhc_sequence\": pmhc_sequence,\n",
    "        \"pmhc_input_ids\": pmhc_input_ids,\n",
    "        \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "        \"binding\": 0\n",
    "    })\n",
    "batch_binding_dataset = datasets.Dataset.from_pandas(pd.DataFrame(binding_data))\n",
    "batch_binding_dataset = batch_binding_dataset.shuffle()\n",
    "\n",
    "import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.trange(0, len(batch_binding_dataset), n_per_batch):\n",
    "        tcr_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_input_ids']).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_input_ids']).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        binding =  torch.tensor(batch_binding_dataset[i:i+n_per_batch]['binding']).to(\"cuda:0\").to(torch.float32).unsqueeze(1)\n",
    "\n",
    "        output2 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_binding=binding\n",
    "        )\n",
    "        all_binding_predictions.append(\n",
    "            output2['contact_output'][3].detach().cpu().numpy()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame([\n",
    "    (np.vstack(all_binding_predictions).flatten() > 0) == (np.array(batch_binding_dataset['binding']) == 1),\n",
    "    list(map(lambda x: x.split(\":\")[-1].split(\".\")[0], batch_binding_dataset['pmhc_sequence']))\n",
    "], index=['correct','peptide']).T.groupby(\"peptide\").agg({\"correct\": Counter})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248228cf",
   "metadata": {},
   "source": [
    "## Test Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binding_predictions = []\n",
    "\n",
    "indices = list(map(lambda x: np.random.choice(range(len(x)), len(x), replace=False), agg_test['tcr_pseudosequence']))\n",
    "binding_data = []\n",
    "batch_binding_data =  FLATTEN([list(zip(np.array(x)[i], np.array(y)[i])) for x,y,i in zip(\n",
    "    agg_test['tcr_pseudosequence'],\n",
    "    agg_test['pmhc_pseudosequence'], \n",
    "    indices\n",
    ")])\n",
    "\n",
    "for tcr_pseudoseq,pmhc_pseudoseq in batch_binding_data:\n",
    "    tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "    pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "    tcr_input_ids = np.array(tcr_input_ids)\n",
    "    tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "    pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "    pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "    binding_data.append({\n",
    "        \"tcr_input_ids\": tcr_input_ids,\n",
    "        \"tcr_attention_mask\": tcr_attention_mask,\n",
    "        \"pmhc_sequence\": pmhc_sequence,\n",
    "        \"pmhc_input_ids\": pmhc_input_ids,\n",
    "        \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "        \"binding\": 1\n",
    "    })\n",
    "\n",
    "    avpseudo = ':'.join(tcr_pseudoseq.split(\":\")[:2])\n",
    "    bvpseudo = ':'.join(tcr_pseudoseq.split(\":\")[3:5])\n",
    "    if avpseudo in df_scrna_avpseudo_index:\n",
    "        indices = df_scrna_avpseudo_index[avpseudo]\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "    if bvpseudo in df_scrna_bvpseudo_index:\n",
    "        indices = df_scrna_bvpseudo_index[bvpseudo]\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices.flatten()))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "    if avpseudo in df_scrna_avpseudo_index and bvpseudo in df_scrna_bvpseudo_index:\n",
    "        indices = list(set(df_scrna_avpseudo_index[avpseudo]).intersection(set(df_scrna_bvpseudo_index[bvpseudo])))\n",
    "        if len(indices) > 0:\n",
    "            decoy_data = ds_scrna[int(np.random.choice(indices))]\n",
    "            tcr_input_ids = decoy_data['input_ids']\n",
    "            tcr_attention_mask = decoy_data['attention_mask']\n",
    "            binding_data.append({\n",
    "                \"tcr_input_ids\": tcr_input_ids,\n",
    "                \"tcr_attention_mask\": tcr_attention_mask,\n",
    "                \"pmhc_sequence\": pmhc_sequence,\n",
    "                \"pmhc_input_ids\": pmhc_input_ids,\n",
    "                \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "                \"binding\": 0\n",
    "            })\n",
    "\n",
    "batch_decoy = ds_scrna[np.random.choice(np.arange(len(ds_scrna)), len(batch_binding_data), replace=False)]\n",
    "\n",
    "for tcr_input_ids, tcr_attention_mask, pmhc_pseudoseq in zip(batch_decoy['input_ids'],batch_decoy['attention_mask'],list(map(lambda x: x[1], batch_binding_data))):\n",
    "    tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "    binding_data.append({\n",
    "        \"tcr_input_ids\": tcr_input_ids,\n",
    "        \"tcr_attention_mask\": tcr_attention_mask,\n",
    "        \"pmhc_sequence\": pmhc_sequence,\n",
    "        \"pmhc_input_ids\": pmhc_input_ids,\n",
    "        \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "        \"binding\": 0\n",
    "    })\n",
    "batch_binding_dataset = datasets.Dataset.from_pandas(pd.DataFrame(binding_data))\n",
    "batch_binding_dataset = batch_binding_dataset.shuffle()\n",
    "\n",
    "import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.trange(0, len(batch_binding_dataset), n_per_batch):\n",
    "        tcr_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_input_ids']).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_input_ids']).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "        binding =  torch.tensor(batch_binding_dataset[i:i+n_per_batch]['binding']).to(\"cuda:0\").to(torch.float32).unsqueeze(1)\n",
    "\n",
    "        output2 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_binding=binding\n",
    "        )\n",
    "        all_binding_predictions.append(\n",
    "            output2['contact_output'][3].detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "pd.DataFrame([\n",
    "    (np.vstack(all_binding_predictions).flatten() > 0) == (np.array(batch_binding_dataset['binding']) == 1),\n",
    "    list(map(lambda x: x.split(\":\")[-1].split(\".\")[0], batch_binding_dataset['pmhc_sequence']))\n",
    "], index=['correct','peptide']).T.groupby(\"peptide\").agg({\"correct\": Counter})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a233bc0",
   "metadata": {},
   "source": [
    "## Extract attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83392bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attentions_tcr = []\n",
    "all_attentions_pmhc = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.trange(0, len(contact_dataset)):\n",
    "        tcr_input_ids = torch.from_numpy(contact_dataset[i]['tcr_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        tcr_attention_mask = torch.from_numpy(contact_dataset[i]['tcr_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        pmhc_input_ids = torch.from_numpy(contact_dataset[i]['pmhc_input_ids']).unsqueeze(0).to(\"cuda:0\")\n",
    "        pmhc_attention_mask = torch.from_numpy(contact_dataset[i]['pmhc_attention_mask']).unsqueeze(0).to(torch.float32).to(\"cuda:0\")\n",
    "        distogram = torch.from_numpy(contact_dataset[i]['distogram']).unsqueeze(0).to(\"cuda:0\")\n",
    "        output1 = model(\n",
    "            tcr_input_ids=tcr_input_ids,\n",
    "            tcr_attention_mask=tcr_attention_mask,\n",
    "            pmhc_input_ids=pmhc_input_ids,\n",
    "            pmhc_attention_mask=pmhc_attention_mask,\n",
    "            tcr_pmhc_distogram=distogram,\n",
    "            output_triangular_attentions=True\n",
    "        )\n",
    "        all_attentions_tcr.append([output1['contact_output'][-1][j][1].detach().cpu() for j in range(config.num_hidden_layers)])\n",
    "        all_attentions_pmhc.append([output1['contact_output'][-1][j][0].detach().cpu() for j in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "all_attentions_tcr = [torch.vstack([ x[i] for x in all_attentions_tcr]) for i in range(config.num_hidden_layers)]\n",
    "all_attentions_pmhc = [torch.vstack([ x[i] for x in all_attentions_pmhc]) for i in range(config.num_hidden_layers)]\n",
    "# Layer, Batch, pMHC pseudo, Head, TCR pseudo, TCR pseudo\n",
    "all_attentions_tcr = torch.vstack(list(map(lambda x: x.unsqueeze(0), all_attentions_tcr)))\n",
    "# Layer, Batch, TCR pseudo, Head, pMHC pseudo, pMHC pseudo\n",
    "all_attentions_pmhc = torch.vstack(list(map(lambda x: x.unsqueeze(0), all_attentions_pmhc)))\n",
    "\n",
    "# all_attentions[0].mean([0,1,2,3,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "binding_data = []\n",
    "n_per_batch = 10\n",
    "for tcr_pseudoseqs,pmhc_pseudoseqs in zip(agg['tcr_pseudosequence'], agg['pmhc_pseudosequence']):\n",
    "    for tcr_pseudoseq,pmhc_pseudoseq in zip(tcr_pseudoseqs,pmhc_pseudoseqs):\n",
    "        tcr_sequence, tcr_input_ids, tcr_attention_mask = tokenize_tcr_pseudo_sequence_to_fixed_length(tcr_pseudoseq)\n",
    "        pmhc_sequence, pmhc_input_ids, pmhc_attention_mask = tokenize_to_fixed_length(pmhc_pseudoseq, 50)\n",
    "        tcr_input_ids = np.array(tcr_input_ids)\n",
    "        tcr_attention_mask = np.array(tcr_attention_mask)\n",
    "        pmhc_input_ids = np.array(pmhc_input_ids)\n",
    "        pmhc_attention_mask = np.array(pmhc_attention_mask)\n",
    "        binding_data.append({\n",
    "            \"tcr_input_ids\": tcr_input_ids,\n",
    "            \"tcr_attention_mask\": tcr_attention_mask,\n",
    "            \"pmhc_sequence\": pmhc_sequence,\n",
    "            \"pmhc_input_ids\": pmhc_input_ids,\n",
    "            \"pmhc_attention_mask\": pmhc_attention_mask,\n",
    "            \"binding\": 1\n",
    "        })\n",
    "\n",
    "all_batch_binding_dataset = datasets.Dataset.from_pandas(pd.DataFrame(binding_data))\n",
    "\n",
    "for pmhc in np.unique(all_batch_binding_dataset['pmhc_sequence']):\n",
    "    batch_binding_dataset = all_batch_binding_dataset.select( np.argwhere(np.array(list(map(lambda x: x['pmhc_sequence'] == pmhc, all_batch_binding_dataset)) )))\n",
    "\n",
    "    all_attentions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.trange(0, len(batch_binding_dataset), n_per_batch):\n",
    "            tcr_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_input_ids']).to(\"cuda:0\")\n",
    "            tcr_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['tcr_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "            pmhc_input_ids = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_input_ids']).to(\"cuda:0\")\n",
    "            pmhc_attention_mask = torch.tensor(batch_binding_dataset[i:i+n_per_batch]['pmhc_attention_mask']).to(torch.float32).to(\"cuda:0\")\n",
    "            binding =  torch.tensor(batch_binding_dataset[i:i+n_per_batch]['binding']).to(\"cuda:0\").to(torch.float32).unsqueeze(1)\n",
    "            output2 = model(\n",
    "                tcr_input_ids=tcr_input_ids,\n",
    "                tcr_attention_mask=tcr_attention_mask,\n",
    "                pmhc_input_ids=pmhc_input_ids,\n",
    "                pmhc_attention_mask=pmhc_attention_mask,\n",
    "                tcr_pmhc_binding=binding,\n",
    "                output_triangular_attentions=True\n",
    "            )\n",
    "            attention = torch.vstack([output2['contact_output'][-1][j][1].detach().cpu().unsqueeze(0) for j in range(config.num_hidden_layers)])\n",
    "            np.save(f'./data/attention_score/{pmhc.split(\":\")[-1].replace(\".\",\"\")}_{i}.npy', attention.detach().cpu().numpy())\n",
    "            del output2\n",
    "            torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-faiss-gpu",
   "language": "python",
   "name": "python3-faiss-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
